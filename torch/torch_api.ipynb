{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.Tensor([[[[1,1,1],[2,2,2]]]])\n",
    "b = torch.Tensor([[[[3,3,3],[4,4,4]]]])\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.concat((a,b), dim = 0)\n",
    "print(c.shape)\n",
    "c = torch.concat((a,b), dim = 3)\n",
    "c.shape, c\n",
    "# concat时除了dim维度，其他维度shape需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suqeeze\n",
    "a = torch.Tensor([[[[1,1,1,1],[2,2,2,2]]]])\n",
    "b = torch.Tensor([[[[3,3,3],[4,4,4]]]])\n",
    "c = torch.squeeze(a)\n",
    "print(c.shape)\n",
    "d = torch.squeeze(a, dim = 2)\n",
    "# 只能suqeeze dim 维度 只有一个元素的， 否则squeeze失效\n",
    "print(a.shape, d.shape)\n",
    "d = torch.squeeze(a, dim = 0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsqueeze 在dim 维度插入\n",
    "c = torch.unsqueeze(a, dim = 3)\n",
    "print(a.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack\n",
    "a = torch.Tensor([[[[1,1,1],[2,2,2]]]])\n",
    "b = torch.Tensor([[[[3,3,3],[4,4,4]]]])\n",
    "print(a.shape, b.shape)\n",
    "# stack 添加一个dim维度，要求输入shape相同\n",
    "c = torch.stack((a,b), dim = 3)\n",
    "print(c.shape, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x.shape)\n",
    "# 最后一个维度和原来维度对齐\n",
    "expanded_x = x.expand(2, 3)  # 扩展为3x3的矩阵\n",
    "\n",
    "print(expanded_x.shape)\n",
    "\n",
    "# 修改扩展后的张量\n",
    "expanded_x[0, 0] = 10\n",
    "\n",
    "print(x)  # 输出: tensor([10,  2,  3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,1,1], [2,2,2]],[[3,3,3],[4,4,4]]])\n",
    "b = torch.tensor([[[5,5,5], [6,6,6]],[[7,7,7],[8,8,8]]])\n",
    "# tensor([[[1, 1, 1],\n",
    "#          [2, 2, 2]],\n",
    "\n",
    "#         [[3, 3, 3],\n",
    "#          [4, 4, 4]]])\n",
    "\n",
    "# tensor([[[5, 6],\n",
    "#          [5, 6],\n",
    "#          [5, 6]],\n",
    "\n",
    "#         [[7, 8],\n",
    "#          [7, 8],\n",
    "#          [7, 8]]])\n",
    "print(b.transpose(1,2))\n",
    "\n",
    "c = torch.matmul(a, b.transpose(1,2))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Linear\n",
    "#torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "from torch import nn\n",
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())\n",
    "print(output.shape[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(12).reshape(1, 6, 2)\n",
    "# 多切不会有问题 dim=0 维度只有1\n",
    "split_a = torch.split(a, 2)\n",
    "print(split_a)\n",
    "split_a = torch.split(a, 2, dim = 1)\n",
    "print(split_a[0].shape) # 1, 2, 2\n",
    "b = torch.arange(10).reshape(5, 2)\n",
    "# dim=0维 切分 shape [5, 2] --> [1, 2] [2, 2] [2, 2]\n",
    "# list 必须表示该维度的切分策略， list 元素之和必须等于该维度\n",
    "torch.split(b, [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2553, -0.1690,  0.1118],\n",
      "        [ 0.7074, -2.9165,  0.0750]])\n",
      "tensor([[ 0.2553,  0.7074],\n",
      "        [-0.1690, -2.9165],\n",
      "        [ 0.1118,  0.0750]])\n",
      "tensor([[ 0.2553, -0.1690,  0.1118],\n",
      "        [ 0.7074, -2.9165,  0.0750]])\n",
      "tensor([[ 2.0000,  0.7074],\n",
      "        [-0.1690, -2.9165],\n",
      "        [ 0.1118,  0.0750]])\n",
      "x and y share memory: False\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor.contiguous\n",
    "# Tensor.contiguous(memory_format=torch.contiguous_format) → Tensor\n",
    "# Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.transpose(x, 0, 1).contiguous()\n",
    "print(x)\n",
    "print(y)\n",
    "y[0][0] = 2\n",
    "print(x)\n",
    "print(y)\n",
    "# contiguous 调用后返回新的tensor\n",
    "print(\"x and y share memory:\", x.data_ptr() == y.data_ptr()) # False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
